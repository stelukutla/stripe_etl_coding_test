Use Informatica to build an Integration solution to fetch dataset for SFDC object  and transform it into parquet format. Place the parquet file on S3 bucket 
hence writing a python script to consume data from S3 and publish to redshift.

Expected outcome of this exercise
1. Informatica workflow with components for this integration
2. Capture exception and send notification to configured email address
3. Retrigger transaction for error out flow
4. Python script code to consume data from S3 to Redshift that runs once a day for multiple file ingestion
5. Parameterize workflow as well as script where required.

Sankara Solution 
--------------------------------------------------------------------------------------------------------------
Highlevel Compents ans Pseudo Algorithm ( Files and Screenshots attached for most of the cases)
Component Used: 
-- Informatica Connectors
   Informatica SFDC
   Informatica S3 V2 
   

Informatica Cloud features Used: 
 -- Mapping
 -- Mapping Task
 -- Scheduling
 -- Email Configuration
 -- Fault tolerance

 -- Parameterize Fields 
     -- For Task1, Retrive SFDC object data to S3 (INFA will convert to parquet file)
          - S3 Bucket location, S3 keys, job frequency, email_sender,email receivers list, SFDC Object Name
     -- For Task2, Retrive S3 parquet files from processed folder and retrive using pandas dataframe to store to redshift
          - Task 1 params and Redshift connecition params.

-- Mapping Task 1
 - connect to SFDC
 - Get the object data
 - Convert SFDC Object to parquet by the changind the target setting ( Screenshot attached)
 - Send the file to S3

-- Mapping , Mapping Task 2 and Scheduling
 - while ( each day)
     - for every file in the day {
     - S3ToRedshift_Task
        - Python script code to consume data from S3 to Redshift  ( Code attached ) 
     
 - Python packages used
    - pandas : Used to convert the parquet file and filter based columns needed
    - logging : Used for logging the activities during the execution
    - sqlalchemy : SQLAlchemy is the Python SQL toolkit for Redshift and Object Relational Mapper that gives application developers the full power and flexibility of SQL.
    - boto3 : boto3 used to list the s3 objects and download the file
    - os : Used to create a folder which is used to store the received files
    
  - Tools/Softwares used
   - Ananconda with python 3.7
   - Informatica Cloud
   - Salesforce Cloud
   - Python 3.7
   - Pandas
   - Git
   - Amazon S3
   - Amazon Redshift
   
 
