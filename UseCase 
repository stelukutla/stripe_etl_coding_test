Use Informatica to build an Integration solution t
   o fetch dataset for SFDC object 
   and transform it into parquet format. 
   
   Place the parquet file on S3 bucket 
      hence writing a python script to consume data from S3 
      and publish to redshift.

Expected outcome of this exercise

1. Informatica workflow with components for this integration

2. Capture exception and send notification to configured email address

3. Retrigger transaction for error out flow

4. Python script code to consume data from S3 to Redshift that runs once a day for multiple file ingestion

5. Parameterize workflow as well as script where required.



-- Informatica Connectors
   Informatica SFDC
   Informatica S3 V2 
   Informatica Redshift

 -- Mapping
 -- Mapping Task
 -- Scheduling
 -- Email Configuration
 -- Fault tolerance

 -- Parameterize Fields 
     -- TODO

-- Mapping Task 1
 - connect to SFDC
 - Get the object data
 - Convert CSV to parquet
 - send the file to S3

-- Mapping , Mapping Task and Scheduling
 - while ( each day)
   for every file in the day {
    S3ToRedshift_Task
     Python script code to consume data from S3 to Redshift
  

